{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Dropout, LSTM\n",
    "\n",
    "import cv2 #python -m pip install opencv-python\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import math\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_data(img_rows, img_cols):\n",
    "    # Load cifar10 training and test sets\n",
    "    (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "    # Resize training images\n",
    "    X_train = np.array([cv2.resize(img, (img_rows, img_cols)) for img in X_train[:, :, :, :]])\n",
    "    X_test = np.array([cv2.resize(img, (img_rows, img_cols)) for img in X_test[:, :, :, :]])\n",
    "\n",
    "    X_train = X_train.astype('float16') / 255.0\n",
    "    X_test = X_test.astype('float16') / 255.0\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2] * X_train.shape[3]))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2] * X_test.shape[3]))                       \n",
    "\n",
    "    # Transform targets to keras compatible format\n",
    "    Y_train = to_categorical(Y_train, num_classes)\n",
    "    Y_test = to_categorical(Y_test, num_classes)\n",
    "\n",
    "    print(\"X_train: {0}\".format(X_train.shape))\n",
    "    print(\"Y_train: {0}\".format(Y_train.shape))\n",
    "    print(\"X_test: {0}\".format(X_test.shape))\n",
    "    print(\"Y_test: {0}\".format(Y_test.shape))\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 224, 672)\n",
      "Y_train: (50000, 10)\n",
      "X_test: (10000, 224, 672)\n",
      "Y_test: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_cifar10_data(224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_size=224\n",
    "feature_size=224*3\n",
    "\n",
    "kernel_init = tf.keras.initializers.glorot_uniform()\n",
    "bias_init = tf.keras.initializers.Constant(value=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model():\n",
    "    input_layer = Input(batch_shape=(None, series_size, feature_size), name=\"state\")\n",
    "    \n",
    "    rnn_1 = LSTM(\n",
    "        units=256,\n",
    "        input_shape=(series_size, feature_size),  # (타임스텝, 속성)\n",
    "        activation='relu',\n",
    "        dropout=0.5,\n",
    "        stateful=True,\n",
    "        kernel_initializer=kernel_init,\n",
    "        bias_initializer=bias_init,\n",
    "        return_sequences=True)(input_layer)\n",
    "\n",
    "    rnn_2 = LSTM(\n",
    "        units=256,\n",
    "        activation=\"relu\",\n",
    "        dropout=0.5,\n",
    "        stateful=True,\n",
    "        kernel_initializer=kernel_init,\n",
    "        bias_initializer=bias_init,\n",
    "        return_sequences=False)(rnn_1)\n",
    "    \n",
    "    hidden_layer = Dense(\n",
    "        units=128, \n",
    "        activation='relu', \n",
    "        kernel_initializer=kernel_init,\n",
    "        bias_initializer=bias_init)(rnn_2)\n",
    "    \n",
    "    dropout_layer = Dropout(rate=0.5)(hidden_layer)\n",
    "    \n",
    "    softmax = Dense(units=num_classes, activation='softmax', name='softmax')(dropout_layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=softmax)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0513 00:02:10.615107 140528954898240 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fcee2bfdcc0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "W0513 00:02:10.931151 140528954898240 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fce0c13b748>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = build_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/35\n",
      "50000/50000 [==============================] - 856s 17ms/sample - loss: 69.8404 - accuracy: 0.0985 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/35\n",
      "50000/50000 [==============================] - 855s 17ms/sample - loss: 2.3027 - accuracy: 0.0966 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/35\n",
      "50000/50000 [==============================] - 855s 17ms/sample - loss: 2.3027 - accuracy: 0.0962 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/35\n",
      "50000/50000 [==============================] - 860s 17ms/sample - loss: 2.3027 - accuracy: 0.0963 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 5/35\n",
      "50000/50000 [==============================] - 856s 17ms/sample - loss: 2.3027 - accuracy: 0.0966 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 6/35\n",
      "50000/50000 [==============================] - 856s 17ms/sample - loss: 2.3027 - accuracy: 0.0972 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 7/35\n",
      "50000/50000 [==============================] - 850s 17ms/sample - loss: 2.3027 - accuracy: 0.0992 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 8/35\n",
      "50000/50000 [==============================] - 848s 17ms/sample - loss: 2.3027 - accuracy: 0.0981 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 9/35\n",
      "50000/50000 [==============================] - 851s 17ms/sample - loss: 2.3027 - accuracy: 0.0983 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 10/35\n",
      "50000/50000 [==============================] - 859s 17ms/sample - loss: 2.3027 - accuracy: 0.0993 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 11/35\n",
      "50000/50000 [==============================] - 852s 17ms/sample - loss: 2.3027 - accuracy: 0.0988 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 12/35\n",
      "50000/50000 [==============================] - 857s 17ms/sample - loss: 2.3027 - accuracy: 0.0980 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 13/35\n",
      "50000/50000 [==============================] - 853s 17ms/sample - loss: 2.3027 - accuracy: 0.0986 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 14/35\n",
      "50000/50000 [==============================] - 859s 17ms/sample - loss: 2.3027 - accuracy: 0.0972 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 15/35\n",
      "50000/50000 [==============================] - 852s 17ms/sample - loss: 2.3027 - accuracy: 0.0988 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 16/35\n",
      "50000/50000 [==============================] - 854s 17ms/sample - loss: 2.3027 - accuracy: 0.0989 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 17/35\n",
      "50000/50000 [==============================] - 857s 17ms/sample - loss: 2.3027 - accuracy: 0.0983 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 18/35\n",
      "50000/50000 [==============================] - 857s 17ms/sample - loss: 2.3027 - accuracy: 0.0962 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 19/35\n",
      "50000/50000 [==============================] - 848s 17ms/sample - loss: 2.3027 - accuracy: 0.0984 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 20/35\n",
      "50000/50000 [==============================] - 858s 17ms/sample - loss: 2.3027 - accuracy: 0.0997 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 21/35\n",
      "50000/50000 [==============================] - 854s 17ms/sample - loss: 2.3027 - accuracy: 0.0982 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 22/35\n",
      "50000/50000 [==============================] - 851s 17ms/sample - loss: 2.3027 - accuracy: 0.0976 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 23/35\n",
      "50000/50000 [==============================] - 850s 17ms/sample - loss: 2.3027 - accuracy: 0.0974 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 24/35\n",
      "50000/50000 [==============================] - 850s 17ms/sample - loss: 2.3027 - accuracy: 0.0978 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 25/35\n",
      "50000/50000 [==============================] - 848s 17ms/sample - loss: 2.3027 - accuracy: 0.0981 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 26/35\n",
      "50000/50000 [==============================] - 851s 17ms/sample - loss: 2.3027 - accuracy: 0.0986 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 27/35\n",
      "50000/50000 [==============================] - 849s 17ms/sample - loss: 2.3027 - accuracy: 0.0991 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 28/35\n",
      "50000/50000 [==============================] - 852s 17ms/sample - loss: 2.3027 - accuracy: 0.0986 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 29/35\n",
      "50000/50000 [==============================] - 844s 17ms/sample - loss: 2.3027 - accuracy: 0.0991 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 30/35\n",
      "50000/50000 [==============================] - 850s 17ms/sample - loss: 2.3027 - accuracy: 0.0982 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 31/35\n",
      "50000/50000 [==============================] - 852s 17ms/sample - loss: 2.3027 - accuracy: 0.0981 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 32/35\n",
      "50000/50000 [==============================] - 847s 17ms/sample - loss: 2.3027 - accuracy: 0.0988 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 33/35\n",
      "50000/50000 [==============================] - 848s 17ms/sample - loss: 2.3027 - accuracy: 0.0988 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 34/35\n",
      "50000/50000 [==============================] - 847s 17ms/sample - loss: 2.3027 - accuracy: 0.0976 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 35/35\n",
      "50000/50000 [==============================] - 856s 17ms/sample - loss: 2.3027 - accuracy: 0.0982 - val_loss: 2.3026 - val_accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "initial_lrate = 0.01\n",
    "\n",
    "def decay(epoch, steps=100):\n",
    "    drop = 0.96\n",
    "    epochs_drop = 8\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "lr_sc = LearningRateScheduler(decay, verbose=1)\n",
    "\n",
    "sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "epochs = 35\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=epochs, batch_size=256, callbacks=[lr_sc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
