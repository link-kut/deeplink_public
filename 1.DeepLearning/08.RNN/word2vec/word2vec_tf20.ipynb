{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec with tensorflow 2.0\n",
    "- https://byeongkijeong.github.io/Word2vec-from-scratch-using-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense, Dot, Embedding, Input, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword(불용어) 사전 다운로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yhhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus Preprocessing (말뭉치 전처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트에 대해 모두 소문자로 변경\n",
    "- 정규식(Regex)을 이용하여 숫자/알파벳/공백을 제외하고 전부 제거\n",
    "- sampling_rate 변수는 테스트를 위해 전체문서 중에서 일부만 샘플링해서 사용하려고 할 때 사용하는 값이며 0~1 사이의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_corpus(corpus, sampling_rate=1.0):\n",
    "    if sampling_rate is not 1.0:\n",
    "        corpus = corpus.sample(frac=sampling_rate, replace=False)\n",
    "    corpus = corpus.str.lower()\n",
    "    corpus = corpus.str.replace(r'[^A-Za-z0-9\\s]', ' ', regex=True)\n",
    "    return corpus.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"sample_text.csv\").iloc[:,1] \n",
    "corpus = preprocessing_corpus(corpus, sampling_rate=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['act fire witnesses must be aware of defamation', 'a g calls for infrastructure protection summit']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"sample_text.csv\").iloc[:,1] \n",
    "corpus = preprocessing_corpus(corpus, sampling_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a g calls for infrastructure protection summit', 'aba decides against community broadcasting licence', 'act fire witnesses must be aware of defamation']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Making vocabrary (단어집 구성)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Corpus에서 단어를 추출해서 단어집(Vocabrary)을 구성\n",
    "- top_n_ratio는 Corpus 내에서 단어의 출현 빈도 기준 상위 몇%의 단어들을 이용하여 어휘집을 구성할 건지에 대한 파라미터 (범위는 0~1)\n",
    "- 또한, NLTK에 있는 영어 불용어 사전을 이용하여 불용어 제거\n",
    "- 불용어 및 등장횟수가 적어서 단어집에 포함되지 않은 단어들은 UNK로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_vocab(corpus, top_n_ratio=1.0):\n",
    "    words = np.concatenate(np.core.defchararray.split(corpus)).tolist()\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stopWords]\n",
    "\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    print(counter)\n",
    "    \n",
    "    if top_n_ratio is not 1.0:\n",
    "        counter = Counter(dict(counter.most_common(int(top_n_ratio*len(counter)))))\n",
    "        \n",
    "    unique_words = list(counter) + ['UNK']\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'g': 1, 'calls': 1, 'infrastructure': 1, 'protection': 1, 'summit': 1, 'aba': 1, 'decides': 1, 'community': 1, 'broadcasting': 1, 'licence': 1, 'act': 1, 'fire': 1, 'witnesses': 1, 'must': 1, 'aware': 1, 'defamation': 1})\n",
      "['g', 'calls', 'infrastructure', 'protection', 'summit', 'aba', 'decides', 'community', 'broadcasting', 'licence', 'act', 'fire', 'UNK']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "vocab = making_vocab(corpus, top_n_ratio=0.8)\n",
    "print(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'g': 1, 'calls': 1, 'infrastructure': 1, 'protection': 1, 'summit': 1, 'aba': 1, 'decides': 1, 'community': 1, 'broadcasting': 1, 'licence': 1, 'act': 1, 'fire': 1, 'witnesses': 1, 'must': 1, 'aware': 1, 'defamation': 1})\n",
      "['g', 'calls', 'infrastructure', 'protection', 'summit', 'aba', 'decides', 'community', 'broadcasting', 'licence', 'act', 'fire', 'witnesses', 'must', 'aware', 'defamation', 'UNK']\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "vocab = making_vocab(corpus, top_n_ratio=1.0)\n",
    "print(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing vocabrary (각 단어의 인덱스화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어집을 이용하여 단어를 숫자로, 숫자를 단어로 인덱싱(Indexing) 및 역 인덱싱(Reverse indexing)하는 Lookup 테이블 구성 및 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_indexing(vocab):\n",
    "    word2index = {word:index for index, word in enumerate(vocab)}\n",
    "    index2word = {index:word for word, index in word2index.items()}\n",
    "    return word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g': 0, 'calls': 1, 'infrastructure': 2, 'protection': 3, 'summit': 4, 'aba': 5, 'decides': 6, 'community': 7, 'broadcasting': 8, 'licence': 9, 'act': 10, 'fire': 11, 'witnesses': 12, 'must': 13, 'aware': 14, 'defamation': 15, 'UNK': 16}\n",
      "{0: 'g', 1: 'calls', 2: 'infrastructure', 3: 'protection', 4: 'summit', 5: 'aba', 6: 'decides', 7: 'community', 8: 'broadcasting', 9: 'licence', 10: 'act', 11: 'fire', 12: 'witnesses', 13: 'must', 14: 'aware', 15: 'defamation', 16: 'UNK'}\n"
     ]
    }
   ],
   "source": [
    "word2index, index2word = vocab_indexing(vocab)\n",
    "print(word2index)\n",
    "print(index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Changing word to index in corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전 함수에서 인덱싱 된 단어들을 이용하여, Corpus상의 단어들을 인덱스로 바꿔주는 함수\n",
    "- A:0, B:1, C:2 로 인덱싱 되었다고 할 때, 'A B C A' --> [0,1,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_index_into_corpus(word2index, corpus):\n",
    "    indexed_corpus = []\n",
    "    for doc in corpus:\n",
    "        indexed_corpus.append([word2index[word] if word in word2index else word2index['UNK'] for word in doc.split()])\n",
    "    return indexed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a g calls for infrastructure protection summit', 'aba decides against community broadcasting licence', 'act fire witnesses must be aware of defamation']\n",
      "[[16, 0, 1, 16, 2, 3, 4], [5, 6, 16, 7, 8, 9], [10, 11, 12, 13, 16, 14, 16, 15]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n",
    "indexed_corpus = word_index_into_corpus(word2index=word2index, corpus=corpus)\n",
    "print(indexed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating traning pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습에 사용될 데이터 Pairs 생성\n",
    "- 네거티브 샘플링 훈련 데이터\n",
    "  - Positive sample(주변에 위치하는 단어 그룹) --> 1\n",
    "  - Negative sample(주변에 위치하지 않는 단어 그룹) --> 0\n",
    "\n",
    "- Keras의 skipgrams 사용\n",
    "  - [[1,2,3,4,5,6]] -> [[[2,3], 1], [[2,6], 0]]\n",
    "  - 설정된 Window size 안에 있는 단어끼리는 1, 아닌 단어끼리는 0을 Label로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_wordpairs(indexed_corpus, vocab_size, window_size=4):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for row in indexed_corpus:\n",
    "        x, y = skipgrams(\n",
    "            sequence=row,\n",
    "            vocabulary_size=vocab_size,\n",
    "            window_size=window_size,\n",
    "            negative_samples=1.0,\n",
    "            shuffle=True,\n",
    "            categorical=False,\n",
    "            sampling_table=None,\n",
    "            seed=None\n",
    "        )\n",
    "        X = X + list(x)\n",
    "        Y = Y + list(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 10], [1, 2], [16, 3], [4, 11], [3, 16], [4, 2], [2, 1], [16, 2], [1, 16], [3, 1], [3, 6], [2, 3], [1, 4], [2, 8], [16, 1], [16, 1], [3, 1], [16, 4], [3, 4], [4, 1], [4, 3], [3, 2], [16, 4], [16, 5], [16, 16], [3, 6], [4, 1], [3, 14], [16, 2], [16, 16], [2, 12], [4, 14], [1, 16], [2, 3], [16, 3], [1, 2], [1, 5], [16, 4], [2, 11], [16, 6], [16, 9], [4, 16], [2, 4], [2, 4], [1, 9], [16, 13], [2, 16], [4, 1], [1, 6], [1, 3], [2, 16], [1, 11], [6, 7], [9, 11], [6, 9], [5, 14], [5, 4], [7, 13], [7, 9], [8, 5], [7, 11], [16, 7], [8, 10], [9, 16], [16, 9], [16, 8], [5, 2], [7, 6], [9, 6], [16, 15], [6, 8], [8, 16], [9, 3], [16, 5], [16, 9], [6, 11], [9, 8], [7, 9], [7, 5], [7, 16], [5, 16], [7, 1], [8, 9], [7, 3], [6, 9], [8, 15], [6, 5], [5, 6], [6, 9], [16, 11], [8, 16], [8, 6], [8, 7], [8, 3], [9, 7], [6, 12], [16, 12], [7, 8], [9, 14], [16, 6], [16, 12], [5, 8], [8, 8], [6, 11], [5, 9], [6, 16], [9, 7], [5, 7], [12, 10], [12, 14], [16, 13], [11, 12], [12, 16], [14, 15], [12, 13], [16, 11], [12, 16], [10, 7], [11, 13], [11, 10], [16, 1], [12, 16], [16, 11], [14, 10], [16, 16], [15, 12], [15, 16], [11, 9], [11, 1], [12, 6], [16, 9], [15, 5], [15, 14], [10, 12], [13, 16], [15, 16], [14, 2], [16, 15], [14, 3], [14, 16], [16, 12], [16, 14], [10, 13], [14, 8], [13, 15], [13, 1], [13, 10], [14, 12], [16, 15], [10, 16], [16, 1], [13, 8], [11, 16], [13, 7], [13, 11], [15, 4], [16, 16], [15, 13], [10, 12], [13, 16], [16, 10], [10, 13], [14, 7], [11, 14], [10, 3], [15, 10], [16, 14], [13, 11], [16, 10], [14, 16], [12, 6], [16, 13], [13, 16], [11, 1], [14, 2], [16, 12], [11, 13], [12, 3], [16, 11], [14, 11], [13, 12], [16, 13], [11, 13], [14, 13], [10, 11], [13, 10], [16, 16], [12, 13], [12, 11], [16, 2], [16, 8], [13, 14], [16, 3], [13, 15], [12, 16], [16, 16]]\n"
     ]
    }
   ],
   "source": [
    "X, Y = generating_wordpairs(indexed_corpus=indexed_corpus, vocab_size=vocab_size, window_size=4)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 10] 0\n",
      "UNK act 0\n"
     ]
    }
   ],
   "source": [
    "print(X[0], Y[0])\n",
    "print(index2word[X[0][0]], index2word[X[0][1]], Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2] 0\n",
      "calls infrastructure 0\n"
     ]
    }
   ],
   "source": [
    "print(X[1], Y[1])\n",
    "print(index2word[X[1][0]], index2word[X[1][1]], Y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Constructing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding layer의 입력의 크기는 Vocabrary_size * Embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consructing_model(vocab_size, embedding_dim=300):\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=1)\n",
    "\n",
    "    input_target = Input((1,))\n",
    "    target_embedding = embedding_layer(input_target)\n",
    "    target_embedding = Reshape((embedding_dim, 1))(target_embedding)\n",
    "\n",
    "    input_context = Input((1,))\n",
    "    context_embedding = embedding_layer(input_context)\n",
    "    context_embedding = Reshape((embedding_dim, 1))(context_embedding)\n",
    "\n",
    "    hidden_layer = Dot(axes=1)([target_embedding, context_embedding])\n",
    "    hidden_layer = Reshape((1,))(hidden_layer)\n",
    "\n",
    "    output = Dense(16, activation='sigmoid')(hidden_layer)\n",
    "    output = Dense(1, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "    \n",
    "    nesterov = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=nesterov)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 5)         85          input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_27 (Reshape)            (None, 5, 1)         0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (None, 5, 1)         0           embedding_9[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_9 (Dot)                     (None, 1, 1)         0           reshape_27[0][0]                 \n",
      "                                                                 reshape_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_29 (Reshape)            (None, 1)            0           dot_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 16)           32          reshape_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            17          dense_18[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 134\n",
      "Trainable params: 134\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = consructing_model(vocab_size=vocab_size, embedding_dim=5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Traning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, epochs, batch_size, indexed_corpus, vocab_size):\n",
    "    for i in range(epochs):\n",
    "        idx_batch = np.random.choice(len(indexed_corpus), batch_size)\n",
    "        X, Y = generating_wordpairs(np.array(indexed_corpus)[idx_batch].tolist(), vocab_size)\n",
    "\n",
    "        word_target, word_context = zip(*X)\n",
    "        word_target = np.array(word_target, dtype=np.int32)\n",
    "        word_context = np.array(word_context, dtype=np.int32)\n",
    "\n",
    "        target = np.zeros((1,))\n",
    "        context = np.zeros((1,))\n",
    "        label = np.zeros((1,))\n",
    "        idx = np.random.randint(0, len(Y)-1)\n",
    "        \n",
    "        target[0] = word_target[idx]\n",
    "        context[0] = word_context[idx]\n",
    "        label[0] = Y[idx]\n",
    "        \n",
    "        loss = model.train_on_batch([target, context], label)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"Iteration {}, loss={}\".format(i, loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectors(file_path, vocab_size, embedding_dim, model, word2index):\n",
    "    f = open(file_path, 'w')\n",
    "    f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
    "    vectors = model.get_weights()[0]\n",
    "    for word, i in word2index.items():\n",
    "        f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "    f.close()\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus was loaded\n",
      "Counter({'aba': 1, 'decides': 1, 'community': 1, 'broadcasting': 1, 'licence': 1, 'g': 1, 'calls': 1, 'infrastructure': 1, 'protection': 1, 'summit': 1, 'act': 1, 'fire': 1, 'witnesses': 1, 'must': 1, 'aware': 1, 'defamation': 1})\n",
      "Vocabulary was configured.\n",
      "Vocabulary was indexed\n",
      "Corpus was indexed\n",
      "Model was constructed\n",
      "Iteration 0, loss=1.3329920768737793\n",
      "Iteration 50, loss=0.7628775835037231\n",
      "Iteration 100, loss=0.5817394256591797\n",
      "Iteration 150, loss=0.43404635787010193\n",
      "Iteration 200, loss=0.4107160270214081\n",
      "Iteration 250, loss=0.9342852830886841\n",
      "Iteration 300, loss=1.4558486938476562\n",
      "Iteration 350, loss=0.9269182085990906\n",
      "Iteration 400, loss=0.327515184879303\n",
      "Iteration 450, loss=0.5844373106956482\n",
      "Iteration 500, loss=0.6577849984169006\n",
      "Iteration 550, loss=1.0636420249938965\n",
      "Iteration 600, loss=0.6128472089767456\n",
      "Iteration 650, loss=0.669487714767456\n",
      "Iteration 700, loss=0.31303921341896057\n",
      "Iteration 750, loss=0.47006168961524963\n",
      "Iteration 800, loss=1.0585472583770752\n",
      "Iteration 850, loss=1.354886770248413\n",
      "Iteration 900, loss=0.843743085861206\n",
      "Iteration 950, loss=0.5833856463432312\n",
      "Traning was done\n",
      "Trained vector was saved\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    corpus = pd.read_csv(\"sample_text.csv\").iloc[:,1] \n",
    "    corpus = preprocessing_corpus(corpus, sampling_rate=1.0)\n",
    "    print(\"Corpus was loaded\")\n",
    "    \n",
    "    vocab = making_vocab(corpus, top_n_ratio=0.8)\n",
    "    vocab_size = len(vocab)\n",
    "    print(\"Vocabulary was configured.\")\n",
    "    \n",
    "    word2index, index2word = vocab_indexing(vocab)\n",
    "    print(\"Vocabulary was indexed\")\n",
    "    \n",
    "    indexed_corpus = word_index_into_corpus(word2index, corpus)\n",
    "    print(\"Corpus was indexed\")\n",
    "\n",
    "    embedding_dim = 3\n",
    "    model = consructing_model(vocab_size, embedding_dim=embedding_dim)\n",
    "    print(\"Model was constructed\")\n",
    "    \n",
    "    epochs = 1000\n",
    "    batch_sentence_size = 512\n",
    "    model = training_model(model, epochs, 512, indexed_corpus, vocab_size)\n",
    "    print(\"Traning was done\")\n",
    "\n",
    "    save_path = save_vectors('simple_vectors_on_batch.txt', vocab_size, embedding_dim, model, word2index)\n",
    "    print(\"Trained vector was saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Loading the Trained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "file_name = \"simple_vectors_on_batch.txt\"\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(file_name, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0 aba [ 0.0334734  -0.03408869 -0.00693431]\n",
      "1 decides [ 0.00384599 -0.00858271  0.02241573]\n",
      "2 community [ 0.03848729 -0.04664322  0.0285083 ]\n",
      "3 broadcasting [-0.00137986 -0.02642735 -0.04121242]\n",
      "4 licence [0.03717165 0.02397118 0.02052025]\n",
      "5 g [ 0.02094166 -0.02416403 -0.04791161]\n",
      "6 calls [ 0.01294432 -0.02538341 -0.04600326]\n",
      "7 infrastructure [0.03305928 0.03947913 0.01050359]\n",
      "8 protection [-0.0239745   0.03978599 -0.02449598]\n",
      "9 summit [ 0.00057333 -0.03502933 -0.0381165 ]\n",
      "10 act [ 0.02697607  0.04306481 -0.02527941]\n",
      "11 fire [ 0.02979267 -0.00152512 -0.02395339]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.vector_size)\n",
    "for idx in range(len(word_vectors.index2word)):\n",
    "    word = word_vectors.index2word[idx]\n",
    "    print(idx, word, word_vectors.get_vector(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vectors.vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
