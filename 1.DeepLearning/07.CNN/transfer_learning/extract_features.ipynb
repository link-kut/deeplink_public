{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "\n",
    "# other imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the user configs\n",
    "with open('conf/conf.json') as f:    \n",
    "    config = json.load(f)\n",
    "\n",
    "# config variables\n",
    "model_name    = config[\"model\"]\n",
    "weights     = config[\"weights\"]\n",
    "include_top   = config[\"include_top\"]\n",
    "train_path    = config[\"train_path\"]\n",
    "features_path   = config[\"features_path\"]\n",
    "labels_path   = config[\"labels_path\"]\n",
    "test_size     = config[\"test_size\"]\n",
    "results     = config[\"results\"]\n",
    "model_path    = config[\"model_path\"]\n",
    "seed          = config[\"seed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] start time - 2019-05-14 22:40\n",
      "[INFO] successfully loaded base model and model...\n",
      "train_labels: ['snowdrop', 'lilyvalley', 'bluebell', 'daisy', 'cowslip', 'pansy', 'coltsfoot', 'fritillary', 'buttercup', 'daffodil', 'crocus', 'tulip', 'windflower', 'sunflower', 'tigerlily', 'iris', 'dandelion']\n",
      "[INFO] encoding labels...\n",
      "[INFO] completed image & label - snowdrop\n",
      "[INFO] completed image & label - lilyvalley\n",
      "[INFO] completed image & label - bluebell\n",
      "[INFO] completed image & label - daisy\n",
      "[INFO] completed image & label - cowslip\n",
      "[INFO] completed image & label - pansy\n",
      "[INFO] completed image & label - coltsfoot\n",
      "[INFO] completed image & label - fritillary\n",
      "[INFO] completed image & label - buttercup\n",
      "[INFO] completed image & label - daffodil\n",
      "[INFO] completed image & label - crocus\n",
      "[INFO] completed image & label - tulip\n",
      "[INFO] completed image & label - windflower\n",
      "[INFO] completed image & label - sunflower\n",
      "[INFO] completed image & label - tigerlily\n",
      "[INFO] completed image & label - iris\n",
      "[INFO] completed image & label - dandelion\n",
      "[STATUS] training labels: [12 12 12 ...  7  7  7]\n",
      "[STATUS] training labels shape: (1360,)\n",
      "[STATUS] end time - 2019-05-14 22:40\n"
     ]
    }
   ],
   "source": [
    "# start time\n",
    "print(\"[STATUS] start time - {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")))\n",
    "start = time.time()\n",
    "\n",
    "# create the pretrained models\n",
    "# check for pretrained weight usage or not\n",
    "# check for top layers to be included or not\n",
    "image_size = (299, 299)\n",
    "\n",
    "if model_name == \"inceptionv3\":\n",
    "    base_model = InceptionV3(\n",
    "        include_top=include_top, \n",
    "        weights=weights, \n",
    "        input_tensor=Input(shape=(299, 299, 3))\n",
    "    )\n",
    "    \n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "elif model_name == \"xception\":\n",
    "    base_model = Xception(\n",
    "        weights=weights\n",
    "    )\n",
    "    \n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.get_layer('avg_pool').output\n",
    "    \n",
    "else:\n",
    "    base_model = None\n",
    "    \n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer\n",
    "\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"[INFO] successfully loaded base model and model...\")\n",
    "\n",
    "# path to training dataset\n",
    "train_labels = os.listdir(train_path)\n",
    "\n",
    "print(\"train_labels:\", train_labels)\n",
    "\n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "le.fit([tl for tl in train_labels])\n",
    "\n",
    "# variables to hold features and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# loop over all the labels in the folder\n",
    "count = 1\n",
    "\n",
    "for i, label in enumerate(train_labels):\n",
    "    cur_path = train_path + \"/\" + label\n",
    "    count = 1\n",
    "    for image_path in glob.glob(cur_path + \"/*.jpg\"):\n",
    "        img = image.load_img(image_path, target_size=image_size)\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        \n",
    "        images.append(x)\n",
    "        labels.append(label)\n",
    "        count += 1\n",
    "    print(\"[INFO] completed image & label - \" + label)\n",
    "\n",
    "# encode the labels using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le_labels = le.fit_transform(labels)\n",
    "\n",
    "# get the shape of training labels\n",
    "print(\"[STATUS] training labels: {}\".format(le_labels))\n",
    "print(\"[STATUS] training labels shape: {}\".format(le_labels.shape))\n",
    "\n",
    "# end time\n",
    "end = time.time()\n",
    "print(\"[STATUS] end time - {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] images shape: (1360, 299, 299, 3)\n",
      "[INFO] labels shape: (1360,)\n"
     ]
    }
   ],
   "source": [
    "images = np.array(images)\n",
    "images = np.squeeze(images, axis=1)\n",
    "labels = np.array(le_labels)\n",
    "# verify the shape of features and labels\n",
    "print(\"[INFO] images shape: {}\".format(images.shape))\n",
    "print(\"[INFO] labels shape: {}\".format(labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training started...\n",
      "[16  5 14 ...  8  3  3]\n",
      "[INFO] splitted train and test data...\n",
      "[INFO] train data  : (1224, 299, 299, 3)\n",
      "[INFO] test data   : (136, 299, 299, 3)\n",
      "[INFO] train labels: (1224, 17)\n",
      "[INFO] test labels : (136, 17)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"[INFO] training started...\")\n",
    "# split the training and testing data\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(\n",
    "    images,\n",
    "    labels,\n",
    "    test_size=test_size,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"[INFO] splitted train and test data...\")\n",
    "print(\"[INFO] train data  : {}\".format(x_train.shape))\n",
    "print(\"[INFO] test data   : {}\".format(x_test.shape))\n",
    "print(\"[INFO] train labels: {}\".format(y_train.shape))\n",
    "print(\"[INFO] test labels : {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1224 samples, validate on 136 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/35\n",
      "1224/1224 [==============================] - 13s 11ms/sample - loss: 2.7772 - accuracy: 0.1144 - val_loss: 2.7573 - val_accuracy: 0.1324\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 2/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 2.4641 - accuracy: 0.3840 - val_loss: 2.5317 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 3/35\n",
      "1224/1224 [==============================] - 4s 4ms/sample - loss: 2.0570 - accuracy: 0.6471 - val_loss: 2.2867 - val_accuracy: 0.3824\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 4/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 1.6448 - accuracy: 0.8096 - val_loss: 2.1306 - val_accuracy: 0.3971\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 5/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 1.2812 - accuracy: 0.8742 - val_loss: 1.9777 - val_accuracy: 0.4338\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 6/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.9903 - accuracy: 0.9134 - val_loss: 1.9071 - val_accuracy: 0.4338\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 7/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.7827 - accuracy: 0.9232 - val_loss: 1.9261 - val_accuracy: 0.4412\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 8/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.6262 - accuracy: 0.9314 - val_loss: 1.8912 - val_accuracy: 0.4485\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 9/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.5222 - accuracy: 0.9469 - val_loss: 1.8370 - val_accuracy: 0.4485\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 10/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.4442 - accuracy: 0.9534 - val_loss: 1.8347 - val_accuracy: 0.4632\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 11/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.3800 - accuracy: 0.9567 - val_loss: 1.8536 - val_accuracy: 0.4706\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 12/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.3349 - accuracy: 0.9673 - val_loss: 1.8296 - val_accuracy: 0.4632\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 13/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.3035 - accuracy: 0.9673 - val_loss: 1.8140 - val_accuracy: 0.4853\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 14/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.2714 - accuracy: 0.9714 - val_loss: 1.8216 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0096.\n",
      "Epoch 15/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.2506 - accuracy: 0.9747 - val_loss: 1.8286 - val_accuracy: 0.4926\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 16/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.2295 - accuracy: 0.9788 - val_loss: 1.8247 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 17/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.2112 - accuracy: 0.9837 - val_loss: 1.8490 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 18/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.1932 - accuracy: 0.9845 - val_loss: 1.7985 - val_accuracy: 0.5074\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 19/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.1770 - accuracy: 0.9853 - val_loss: 1.7986 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 20/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.1673 - accuracy: 0.9902 - val_loss: 1.8228 - val_accuracy: 0.5074\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 21/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.1587 - accuracy: 0.9918 - val_loss: 1.8342 - val_accuracy: 0.5147\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 22/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.1494 - accuracy: 0.9902 - val_loss: 1.8233 - val_accuracy: 0.5074\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.009216.\n",
      "Epoch 23/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.1377 - accuracy: 0.9959 - val_loss: 1.8243 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 24/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.1298 - accuracy: 0.9967 - val_loss: 1.8272 - val_accuracy: 0.5074\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 25/35\n",
      "1224/1224 [==============================] - 5s 4ms/sample - loss: 0.1246 - accuracy: 0.9959 - val_loss: 1.8110 - val_accuracy: 0.5147\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 26/35\n",
      "1224/1224 [==============================] - 4s 4ms/sample - loss: 0.1198 - accuracy: 0.9951 - val_loss: 1.8249 - val_accuracy: 0.5147\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 27/35\n",
      "1224/1224 [==============================] - 6s 5ms/sample - loss: 0.1103 - accuracy: 0.9975 - val_loss: 1.8262 - val_accuracy: 0.5147\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 28/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.1063 - accuracy: 0.9975 - val_loss: 1.8333 - val_accuracy: 0.5221\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 29/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.0994 - accuracy: 0.9984 - val_loss: 1.8389 - val_accuracy: 0.5221\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 30/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.0937 - accuracy: 0.9992 - val_loss: 1.8500 - val_accuracy: 0.5221\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.008847359999999999.\n",
      "Epoch 31/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.0900 - accuracy: 0.9992 - val_loss: 1.8208 - val_accuracy: 0.5221\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 32/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.0871 - accuracy: 0.9992 - val_loss: 1.8170 - val_accuracy: 0.5221\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 33/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.0848 - accuracy: 0.9992 - val_loss: 1.8253 - val_accuracy: 0.5368\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 34/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.0822 - accuracy: 0.9984 - val_loss: 1.8301 - val_accuracy: 0.5294\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.008493465599999998.\n",
      "Epoch 35/35\n",
      "1224/1224 [==============================] - 4s 3ms/sample - loss: 0.0795 - accuracy: 0.9984 - val_loss: 1.8558 - val_accuracy: 0.5147\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "initial_lrate = 0.01\n",
    "\n",
    "def decay(epoch, steps=100):\n",
    "    drop = 0.96\n",
    "    epochs_drop = 8\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "lr_sc = LearningRateScheduler(decay, verbose=1)\n",
    "\n",
    "sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "epochs = 35\n",
    "\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=epochs, batch_size=256, callbacks=[lr_sc], verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
